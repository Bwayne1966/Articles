{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bwayne1966/Articles/blob/main/Verbalized_Sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tiDyuTeknx1"
      },
      "source": [
        "# ‚úçÔ∏è Verbalized Sampling: How to Unlock LLM Diversity with a Simple Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîó Useful Links\n",
        "[Arxiv Paper](https://arxiv.org/abs/2510.01171) &nbsp;&nbsp;&nbsp;&nbsp; [Blog Post](simonucl.notion.site/verbalized-sampling) &nbsp;&nbsp;&nbsp;&nbsp; [Github Page](https://github.com/CHATS-lab/verbalize-sampling)\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "[Package](https://pypi.org/project/verbalized-sampling/)\n",
        "\n",
        "### üîó Link to other notebooks\n",
        "\n",
        "* **[Direct vs. Verbalized Sampling](https://colab.research.google.com/drive/1UDk4W5w6gF0dQ9Tpu0sPQethEht51GXL#offline=true&sandboxMode=true):** A head-to-head comparison showing a 2-3x diversity improvement in creative tasks while maintaining quality.\n",
        "* **[Image Generation with VS](https://colab.research.google.com/drive/1J18VJRnrCjIb6sTivY-znb8C3JsLQCIz#offline=true&sandboxMode=true):** A visual comparison for text-to-image generation, showcasing creative diversity.\n",
        "* **[Complete Framework Tutorial](https://colab.research.google.com/drive/1eC0nIUVC1kyANxxzhNib44qmPphdWy9o#offline=true&sandboxMode=true):** A step-by-step guide to using verbalized sampling, covering everything from API basics to advanced features."
      ],
      "metadata": {
        "id": "CHTgD2QYxQkU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yeS1dTDFf24v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "This notebook demonstrates how a **simple prompting technique can boost an LLM's creativity by 2x**. Our method effectively mitigates \"mode collapse\", the tendency for models to generate very similar, boring responses.\n",
        "\n",
        "Our running example will be the task of **Story Writing**, using the prompt: \"**Write a 100-word story about a bear.**\""
      ],
      "metadata": {
        "id": "N2gPdVhYnczz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bG_GLOilbu9g"
      },
      "outputs": [],
      "source": [
        "#@title Config: Put Your OpenAI API Key\n",
        "OPENAI_API_KEY = ''\n",
        "\n",
        "N = 5\n",
        "USER_PROMPT = \"Write a 100-word story about a bear.\"\n",
        "DIVERSITY_TUNING_WEIGHT = 0.05"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "import re\n",
        "import textwrap\n",
        "\n",
        "# Put Your OpenAI API Key\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI()\n",
        "\n",
        "#@title Utility Functions\n",
        "from IPython.display import display, HTML, Markdown, clear_output\n",
        "def display_story_comparison(direct_stories, vs_stories, title):\n",
        "    \"\"\"Display stories in a side-by-side comparison format\"\"\"\n",
        "    display(Markdown(f\"## {title}\"))\n",
        "\n",
        "    # Create HTML table for comparison\n",
        "    html = \"<table border='1' style='width:100%; border-collapse: collapse;'>\"\n",
        "    html += \"<tr><th style='width:50%; padding:10px; background-color:#f0f0f0;'>Direct Prompting</th>\"\n",
        "    html += \"<th style='width:50%; padding:10px; background-color:#e8f4f8;'>Verbalized Sampling (Ours)</th></tr>\"\n",
        "\n",
        "    max_stories = max(len(direct_stories), len(vs_stories))\n",
        "\n",
        "    for i in range(max_stories):\n",
        "        html += \"<tr>\"\n",
        "\n",
        "        # Direct Prompting column\n",
        "        html += \"<td style='vertical-align:top; padding:10px;'>\"\n",
        "        if i < len(direct_stories):\n",
        "            html += f\"<strong>Story {i+1}:</strong><br>\"\n",
        "            html += f\"<div style='font-family: serif; line-height: 1.4;'>{direct_stories[i]}</div>\"\n",
        "        html += \"</td>\"\n",
        "\n",
        "        # Verbalized sampling column\n",
        "        html += \"<td style='vertical-align:top; padding:10px;'>\"\n",
        "        if i < len(vs_stories):\n",
        "            html += f\"<strong>Story {i+1}:</strong><br>\"\n",
        "            html += f\"<div style='font-family: serif; line-height: 1.4;'>{vs_stories[i]}</div>\"\n",
        "        html += \"</td>\"\n",
        "\n",
        "        html += \"</tr>\"\n",
        "\n",
        "    html += \"</table>\"\n",
        "    display(HTML(html))\n",
        "\n",
        "def display_single_story_table(stories, title):\n",
        "    \"\"\"Display a single list of stories in a table format\"\"\"\n",
        "    display(Markdown(f\"## {title}\"))\n",
        "\n",
        "    # Create HTML table\n",
        "    html = \"<table border='1' style='width:100%; border-collapse: collapse;'>\"\n",
        "    html += \"<tr><th style='width:100%; padding:10px; background-color:#f0f0f0;'>Generated Stories</th></tr>\"\n",
        "\n",
        "    for i, story in enumerate(stories):\n",
        "        html += \"<tr>\"\n",
        "        html += \"<td style='vertical-align:top; padding:10px;'>\"\n",
        "        html += f\"<strong>Story {i+1}:</strong><br>\"\n",
        "        html += f\"<div style='font-family: serif; line-height: 1.4;'>{story}</div>\"\n",
        "        html += \"</td>\"\n",
        "        html += \"</tr>\"\n",
        "\n",
        "    html += \"</table>\"\n",
        "    display(HTML(html))"
      ],
      "metadata": {
        "id": "jRU7aT_kUelk",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvc7DHP2ksma"
      },
      "source": [
        "---\n",
        "\n",
        "## Traditional Method: Direct Prompting with High Temperature\n",
        "\n",
        "\n",
        "First, the most common way to generate creative stories is to query LLMs **multiple times** at a **high temperature**.\n",
        "\n",
        "Let's see how it works in practice.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "VwBlquENch1V",
        "outputId": "c8ada948-1656-472b-8f49-668baaf5fab6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AuthenticationError",
          "evalue": "Error code: 401 - {'error': {'message': \"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1032516015.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdirect_responses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4.1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': \"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}"
          ]
        }
      ],
      "source": [
        "direct_responses = []\n",
        "for i in range(5):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4.1\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                # direct prompting\n",
        "                \"content\": USER_PROMPT\n",
        "            }\n",
        "        ],\n",
        "        # high temperature\n",
        "        temperature=1.0,\n",
        "    )\n",
        "    response_text = response.choices[0].message.content\n",
        "    direct_responses.append(response_text)\n",
        "    print(f\"Generated story {i+1} of 5: {response_text}\")\n",
        "\n",
        "clear_output()\n",
        "display_single_story_table(direct_responses, \"Direct Prompting\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b213421"
      },
      "source": [
        "**Takeaway**: We see that even with a high `temperature`, all stories from direct prompting share the same theme, the same core idea, and a similar beginning, showing that direct prompting often leads to repetitive outputs (mode collapse).\n",
        "\n",
        "**So, are LLMs just not creative?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YPPLbP2kxjH"
      },
      "source": [
        "## Our Solution: **Verbalized Sampling** (VS)\n",
        "\n",
        "LLMs can, in fact, be highly creative when prompted correctly. We introduce **Verbalized Sampling (VS)**, a simple method that prompts the model to generate a *distribution* of responses with their *probabilities*.\n",
        "\n",
        "Now, let's rerun the same example to see how VS can mitigate mode collapse and unleash the LLM's true creative potential."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4.1\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            # Verbalized Sampling (ours)\n",
        "            \"content\": \"\"\"You are a helpful assistant. For each query, please generate a set of five possible responses, \\\n",
        "            each within a separate <response> tag. Responses should each include a <text> and a numeric <probability>. \\\n",
        "            Please sample at random from the full distribution.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": USER_PROMPT,\n",
        "        }\n",
        "    ],\n",
        "    temperature=1.0,\n",
        ")\n",
        "\n",
        "# Get the response content\n",
        "response_content = response.choices[0].message.content\n",
        "\n",
        "vs_standard_stories = re.findall(r'<text>(.*?)</text>', response_content, re.DOTALL)\n",
        "\n",
        "display_single_story_table(vs_standard_stories, \"Verbalized Sampling\")"
      ],
      "metadata": {
        "id": "DkM3uVvCPVgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Takeaway:** The results are now much more creative and varied. As you can see, each story starts differently and is more diverse.\n",
        "\n",
        "**But can we go further?**\n"
      ],
      "metadata": {
        "id": "XPwxTZbvlJsp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VS with **Tunable Diversity**\n",
        "\n",
        "Yes! We can push for even greater creativity by introducing a **probability threshold**.\n",
        "\n",
        "This technique asks the model to generate more unique, \"long-tail\" responses. This gives us an effective knob to **tune the diversity** of the final output.\n"
      ],
      "metadata": {
        "id": "ba6-a_FxPmhC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPprZTyhc0qh"
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4.1\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            # Verbalized Sampling (diversity tuning)\n",
        "            \"content\": f\"\"\"You are a helpful assistant. For each query, please generate a set of 5 possible responses, \\\n",
        "            each within a separate <response> tag. Responses should each include a <text> and a numeric <probability>. \\\n",
        "            Please sample at random from the tails of the distribution, such that the probability of each response is less than {str(DIVERSITY_TUNING_WEIGHT)}.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": USER_PROMPT,\n",
        "        }\n",
        "    ],\n",
        "    temperature=1.0,\n",
        ")\n",
        "\n",
        "# Get the response content\n",
        "response_content = response.choices[0].message.content\n",
        "\n",
        "vs_stories = re.findall(r'<text>(.*?)</text>', response_content, re.DOTALL)\n",
        "\n",
        "display_single_story_table(vs_stories, \"Verbalized Sampling (w/ Diversity Tuning)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Takeaway:** This enables an even higher level of creativity!\n",
        "\n",
        "For more interesting and creative stories, try **setting the threshold to 0.05 or 0.01 to further increase the diversity**."
      ],
      "metadata": {
        "id": "Ak8-JKiQlXqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Direct ‚öîÔ∏è VS: A Side-by-Side Comparison\n",
        "\n",
        "In this section, we present a side-by-side comparison to highlight the differences between the direct method and Verbalized Sampling."
      ],
      "metadata": {
        "id": "hZm43l5RpQL3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlHNSS1d1eoW"
      },
      "outputs": [],
      "source": [
        "display_story_comparison(direct_responses, vs_stories, \"Story Generation Comparison\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEf4Mm5A5IoS"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Key Benefits of Verbalized Sampling\n",
        "\n",
        "üì£ **A simple training-free prompt** to mitigate mode collapse\n",
        "\n",
        "üéØ **Tunable Diversity**: Explores the \"tails\" of the distribution for tunable creativity.\n",
        "\n",
        "üìà **Scales with Model Size**: Verbalized Sampling works even better on larger models without sacrificing quality.\n",
        "\n",
        "‚ú® **Various Applications**: Effective for creative writing, social simulation, tweet and blog ideas, brainstorming, lesson plan ideas, etc.\n",
        "\n",
        "### üí° Practical tips for using VS\n",
        "- **Use large or reasoning models**: Works best with models like GPT-5, Claude-4, and Gemini 2.5 Pro.\n",
        "- **Ask for longer outputs if length matters**: The LLM may shorten the final output since it's generated in a single response. Where possible, simply tell it the length you want.\n",
        "- **Provide a JSON schema for reliability**: In some cases, models can fail to follow the required format, causing parsing errors. Providing a `json_schema` or using a `structured_output` feature ensures the output format."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Try it yourself!\n",
        "**Here's the direct copy-paste ready code!**\n",
        "\n",
        "### 1. For Chat Interface\n",
        "Prefix your normal query with:\n",
        "```\n",
        "<instruction>\n",
        "For each query, please generate a set of five possible responses, each within a separate <response> tag.\n",
        "Responses should each include a <text> and a numeric <probability> in JSON format.\n",
        "Please sample at random from the full distribution.\n",
        "</instruction>\n",
        "Write a 100-word story about a bear.\n",
        "```\n",
        "or you can also adjust the diversity level by changing the system prompt to:\n",
        "```\n",
        "<instruction>\n",
        "For each query, please generate a set of five possible responses, each within a separate <response> tag.\n",
        "Responses should each include a <text> and a numeric <probability> in JSON format.\n",
        "Please sample at random from the tails of the distribution, such that the probability of each response is less than 0.10.\n",
        "</instruction>\n",
        "Write a 100-word story about a bear.\n",
        "```\n",
        "\n",
        "### 2. For API Calls/Playgrounds:\n",
        "*System Prompt*\n",
        "```\n",
        "You are a helpful assistant. For each query,\n",
        "please generate a set of five possible responses, each within a separate <response> tag.\n",
        "Responses should each include a <text> and a numeric <probability> in JSON format.\n",
        "Please sample at random from the full distribution.\n",
        "```\n",
        "or you can also adjust the diversity level by changing the system prompt to:\n",
        "```\n",
        "You are a helpful assistant. For each query,\n",
        "please generate a set of five possible responses, each within a separate <response> tag.\n",
        "Responses should each include a <text> and a numeric <probability> in JSON format.\n",
        "Please sample at random from the tails of the distribution, such that the probability of each response is less than 0.10.\n",
        "```\n",
        "\n",
        "*User Prompt*\n",
        "```\n",
        "Write a 100-word story about a bear.\n",
        "```"
      ],
      "metadata": {
        "id": "mIEry_4QkGQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# üîó Useful Links\n",
        "[Arxiv Paper](https://arxiv.org/abs/2510.01171)\n",
        "\n",
        "[Blog Post](simonucl.notion.site/verbalized-sampling)\n",
        "\n",
        "[Github Page](https://github.com/CHATS-lab/verbalize-sampling)\n",
        "\n",
        "[Package](https://pypi.org/project/verbalized-sampling/)"
      ],
      "metadata": {
        "id": "SMiN1j5ykKmf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "etmcoihSNIKU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "verbalize",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}